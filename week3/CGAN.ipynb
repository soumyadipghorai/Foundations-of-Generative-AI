{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d27c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torchvision import datasets, transforms \n",
    "from torchvision.utils import save_image, make_grid\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7823cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "z_dim = 100 \n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "channels = 1 \n",
    "epochs = 50 \n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "os.makedirs('cgan_generated_images', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c5e1e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to .\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to .\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=True, download=True, transform=transform),\n",
    "    batch_size= batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a492f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module): \n",
    "    def __init__(self, z_dim, num_classes, img_shape): \n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.img_shape = img_shape\n",
    "        input_dim = z_dim + num_classes \n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256), # for smaller networks --> can be avoided for deeper nn\n",
    "            nn.ReLU(True), \n",
    "\n",
    "            nn.Linear(256, 512), \n",
    "            nn.BatchNorm1d(512), \n",
    "            nn.ReLU(True), \n",
    "\n",
    "            nn.Linear(512, 1024), \n",
    "            nn.BatchNorm1d(1024), \n",
    "            nn.ReLU(True), \n",
    "\n",
    "            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels): \n",
    "        x = torch.cat([noise, self.label_emb(labels)], dim=1)\n",
    "        img = self.model(x)\n",
    "        img = img.view(x.size(0), *self.img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4c928c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module): \n",
    "    def __init__(self, num_classes, img_shape): \n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        input_dim = int(torch.prod(torch.tensor(img_shape))) + num_classes\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "\n",
    "            nn.Linear(512, 256),  \n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels): \n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        x = torch.cat([img_flat, self.label_emb(labels)], dim=1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e3b9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (channels, image_size, image_size)\n",
    "\n",
    "generator = Generator(z_dim, num_classes, img_shape).to(device)\n",
    "discriminator = Discriminator(num_classes, img_shape).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr = lr, betas = (beta1, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr = lr, betas = (beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6037cc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1] [Batch 0/469]D Loss: 1.3728 | G Loss: 0.5697\n",
      "[Epoch 1/1] [Batch 200/469]D Loss: 1.4121 | G Loss: 0.6444\n",
      "[Epoch 1/1] [Batch 400/469]D Loss: 1.3732 | G Loss: 0.6681\n",
      "[Epoch 2/2] [Batch 0/469]D Loss: 1.3854 | G Loss: 0.6868\n",
      "[Epoch 2/2] [Batch 200/469]D Loss: 1.4259 | G Loss: 0.6584\n",
      "[Epoch 2/2] [Batch 400/469]D Loss: 1.3649 | G Loss: 0.7046\n",
      "[Epoch 3/3] [Batch 0/469]D Loss: 1.4129 | G Loss: 0.6790\n",
      "[Epoch 3/3] [Batch 200/469]D Loss: 1.4306 | G Loss: 0.6403\n",
      "[Epoch 3/3] [Batch 400/469]D Loss: 1.3900 | G Loss: 0.6813\n",
      "[Epoch 4/4] [Batch 0/469]D Loss: 1.3551 | G Loss: 0.7254\n",
      "[Epoch 4/4] [Batch 200/469]D Loss: 1.3966 | G Loss: 0.6772\n",
      "[Epoch 4/4] [Batch 400/469]D Loss: 1.3877 | G Loss: 0.6901\n",
      "[Epoch 5/5] [Batch 0/469]D Loss: 1.4105 | G Loss: 0.7227\n",
      "[Epoch 5/5] [Batch 200/469]D Loss: 1.3695 | G Loss: 0.7134\n",
      "[Epoch 5/5] [Batch 400/469]D Loss: 1.4026 | G Loss: 0.7026\n",
      "[Epoch 6/6] [Batch 0/469]D Loss: 1.3827 | G Loss: 0.7157\n",
      "[Epoch 6/6] [Batch 200/469]D Loss: 1.3590 | G Loss: 0.6844\n",
      "[Epoch 6/6] [Batch 400/469]D Loss: 1.3679 | G Loss: 0.7027\n",
      "[Epoch 7/7] [Batch 0/469]D Loss: 1.3708 | G Loss: 0.7222\n",
      "[Epoch 7/7] [Batch 200/469]D Loss: 1.4031 | G Loss: 0.6750\n",
      "[Epoch 7/7] [Batch 400/469]D Loss: 1.3901 | G Loss: 0.7638\n",
      "[Epoch 8/8] [Batch 0/469]D Loss: 1.3663 | G Loss: 0.7024\n",
      "[Epoch 8/8] [Batch 200/469]D Loss: 1.3908 | G Loss: 0.7172\n",
      "[Epoch 8/8] [Batch 400/469]D Loss: 1.4406 | G Loss: 0.7235\n",
      "[Epoch 9/9] [Batch 0/469]D Loss: 1.3870 | G Loss: 0.7024\n",
      "[Epoch 9/9] [Batch 200/469]D Loss: 1.3631 | G Loss: 0.7086\n",
      "[Epoch 9/9] [Batch 400/469]D Loss: 1.3772 | G Loss: 0.7213\n",
      "[Epoch 10/10] [Batch 0/469]D Loss: 1.3562 | G Loss: 0.7275\n",
      "[Epoch 10/10] [Batch 200/469]D Loss: 1.3422 | G Loss: 0.6962\n",
      "[Epoch 10/10] [Batch 400/469]D Loss: 1.3771 | G Loss: 0.7118\n",
      "[Epoch 11/11] [Batch 0/469]D Loss: 1.3795 | G Loss: 0.7087\n",
      "[Epoch 11/11] [Batch 200/469]D Loss: 1.3591 | G Loss: 0.7135\n",
      "[Epoch 11/11] [Batch 400/469]D Loss: 1.4169 | G Loss: 0.7222\n",
      "[Epoch 12/12] [Batch 0/469]D Loss: 1.3257 | G Loss: 0.7457\n",
      "[Epoch 12/12] [Batch 200/469]D Loss: 1.3845 | G Loss: 0.6925\n",
      "[Epoch 12/12] [Batch 400/469]D Loss: 1.3944 | G Loss: 0.7806\n",
      "[Epoch 13/13] [Batch 0/469]D Loss: 1.3758 | G Loss: 0.7376\n",
      "[Epoch 13/13] [Batch 200/469]D Loss: 1.3875 | G Loss: 0.7011\n",
      "[Epoch 13/13] [Batch 400/469]D Loss: 1.3059 | G Loss: 0.7375\n",
      "[Epoch 14/14] [Batch 0/469]D Loss: 1.3322 | G Loss: 0.7576\n",
      "[Epoch 14/14] [Batch 200/469]D Loss: 1.3215 | G Loss: 0.8098\n",
      "[Epoch 14/14] [Batch 400/469]D Loss: 1.4120 | G Loss: 0.7136\n",
      "[Epoch 15/15] [Batch 0/469]D Loss: 1.3917 | G Loss: 0.7863\n",
      "[Epoch 15/15] [Batch 200/469]D Loss: 1.2351 | G Loss: 0.8566\n",
      "[Epoch 15/15] [Batch 400/469]D Loss: 1.2967 | G Loss: 0.7336\n",
      "[Epoch 16/16] [Batch 0/469]D Loss: 1.2798 | G Loss: 0.6229\n",
      "[Epoch 16/16] [Batch 200/469]D Loss: 1.3012 | G Loss: 0.7448\n",
      "[Epoch 16/16] [Batch 400/469]D Loss: 1.3373 | G Loss: 0.7718\n",
      "[Epoch 17/17] [Batch 0/469]D Loss: 1.3876 | G Loss: 0.6807\n",
      "[Epoch 17/17] [Batch 200/469]D Loss: 1.3239 | G Loss: 0.8307\n",
      "[Epoch 17/17] [Batch 400/469]D Loss: 1.2730 | G Loss: 0.7645\n",
      "[Epoch 18/18] [Batch 0/469]D Loss: 1.3855 | G Loss: 0.8925\n",
      "[Epoch 18/18] [Batch 200/469]D Loss: 1.3746 | G Loss: 0.7759\n",
      "[Epoch 18/18] [Batch 400/469]D Loss: 1.3186 | G Loss: 0.6982\n",
      "[Epoch 19/19] [Batch 0/469]D Loss: 1.3795 | G Loss: 0.8460\n",
      "[Epoch 19/19] [Batch 200/469]D Loss: 1.4286 | G Loss: 0.6342\n",
      "[Epoch 19/19] [Batch 400/469]D Loss: 1.5083 | G Loss: 0.7269\n",
      "[Epoch 20/20] [Batch 0/469]D Loss: 1.3432 | G Loss: 0.7622\n",
      "[Epoch 20/20] [Batch 200/469]D Loss: 1.3600 | G Loss: 0.7569\n",
      "[Epoch 20/20] [Batch 400/469]D Loss: 1.3114 | G Loss: 0.8050\n",
      "[Epoch 21/21] [Batch 0/469]D Loss: 1.3475 | G Loss: 0.7098\n",
      "[Epoch 21/21] [Batch 200/469]D Loss: 1.4433 | G Loss: 0.6042\n",
      "[Epoch 21/21] [Batch 400/469]D Loss: 1.4026 | G Loss: 0.6191\n",
      "[Epoch 22/22] [Batch 0/469]D Loss: 1.4046 | G Loss: 0.7007\n",
      "[Epoch 22/22] [Batch 200/469]D Loss: 1.3042 | G Loss: 0.7302\n",
      "[Epoch 22/22] [Batch 400/469]D Loss: 1.2850 | G Loss: 0.8642\n",
      "[Epoch 23/23] [Batch 0/469]D Loss: 1.3348 | G Loss: 0.7229\n",
      "[Epoch 23/23] [Batch 200/469]D Loss: 1.2583 | G Loss: 0.7916\n",
      "[Epoch 23/23] [Batch 400/469]D Loss: 1.4367 | G Loss: 0.7659\n",
      "[Epoch 24/24] [Batch 0/469]D Loss: 1.3219 | G Loss: 0.6800\n",
      "[Epoch 24/24] [Batch 200/469]D Loss: 1.3898 | G Loss: 0.7598\n",
      "[Epoch 24/24] [Batch 400/469]D Loss: 1.3065 | G Loss: 0.7506\n",
      "[Epoch 25/25] [Batch 0/469]D Loss: 1.3920 | G Loss: 0.9232\n",
      "[Epoch 25/25] [Batch 200/469]D Loss: 1.4488 | G Loss: 0.7866\n",
      "[Epoch 25/25] [Batch 400/469]D Loss: 1.3111 | G Loss: 0.8571\n",
      "[Epoch 26/26] [Batch 0/469]D Loss: 1.4255 | G Loss: 0.7900\n",
      "[Epoch 26/26] [Batch 200/469]D Loss: 1.1830 | G Loss: 0.7795\n",
      "[Epoch 26/26] [Batch 400/469]D Loss: 1.2980 | G Loss: 0.6663\n",
      "[Epoch 27/27] [Batch 0/469]D Loss: 1.2230 | G Loss: 0.7444\n",
      "[Epoch 27/27] [Batch 200/469]D Loss: 1.4164 | G Loss: 0.9816\n",
      "[Epoch 27/27] [Batch 400/469]D Loss: 1.4181 | G Loss: 0.7302\n",
      "[Epoch 28/28] [Batch 0/469]D Loss: 1.4867 | G Loss: 0.6614\n",
      "[Epoch 28/28] [Batch 200/469]D Loss: 1.3018 | G Loss: 0.7233\n",
      "[Epoch 28/28] [Batch 400/469]D Loss: 1.2979 | G Loss: 0.8811\n",
      "[Epoch 29/29] [Batch 0/469]D Loss: 1.1812 | G Loss: 0.8907\n",
      "[Epoch 29/29] [Batch 200/469]D Loss: 1.3462 | G Loss: 0.9063\n",
      "[Epoch 29/29] [Batch 400/469]D Loss: 1.3089 | G Loss: 0.8039\n",
      "[Epoch 30/30] [Batch 0/469]D Loss: 1.3650 | G Loss: 0.6397\n",
      "[Epoch 30/30] [Batch 200/469]D Loss: 1.3222 | G Loss: 0.8142\n",
      "[Epoch 30/30] [Batch 400/469]D Loss: 1.2682 | G Loss: 0.9403\n",
      "[Epoch 31/31] [Batch 0/469]D Loss: 1.3846 | G Loss: 0.7999\n",
      "[Epoch 31/31] [Batch 200/469]D Loss: 1.3391 | G Loss: 0.5470\n",
      "[Epoch 31/31] [Batch 400/469]D Loss: 1.4422 | G Loss: 1.0752\n",
      "[Epoch 32/32] [Batch 0/469]D Loss: 1.1359 | G Loss: 0.9390\n",
      "[Epoch 32/32] [Batch 200/469]D Loss: 1.1543 | G Loss: 1.1102\n",
      "[Epoch 32/32] [Batch 400/469]D Loss: 1.1901 | G Loss: 0.8451\n",
      "[Epoch 33/33] [Batch 0/469]D Loss: 1.2665 | G Loss: 0.9509\n",
      "[Epoch 33/33] [Batch 200/469]D Loss: 1.3168 | G Loss: 1.2436\n",
      "[Epoch 33/33] [Batch 400/469]D Loss: 1.2284 | G Loss: 0.9553\n",
      "[Epoch 34/34] [Batch 0/469]D Loss: 1.3391 | G Loss: 0.8491\n",
      "[Epoch 34/34] [Batch 200/469]D Loss: 1.1816 | G Loss: 0.8493\n",
      "[Epoch 34/34] [Batch 400/469]D Loss: 1.2366 | G Loss: 0.7573\n",
      "[Epoch 35/35] [Batch 0/469]D Loss: 1.2258 | G Loss: 0.9247\n",
      "[Epoch 35/35] [Batch 200/469]D Loss: 1.2246 | G Loss: 1.0402\n",
      "[Epoch 35/35] [Batch 400/469]D Loss: 1.2815 | G Loss: 0.9005\n",
      "[Epoch 36/36] [Batch 0/469]D Loss: 1.1927 | G Loss: 0.8521\n",
      "[Epoch 36/36] [Batch 200/469]D Loss: 1.2795 | G Loss: 1.3817\n",
      "[Epoch 36/36] [Batch 400/469]D Loss: 1.3207 | G Loss: 0.6995\n",
      "[Epoch 37/37] [Batch 0/469]D Loss: 1.3721 | G Loss: 1.1009\n",
      "[Epoch 37/37] [Batch 200/469]D Loss: 1.2829 | G Loss: 0.7478\n",
      "[Epoch 37/37] [Batch 400/469]D Loss: 1.2760 | G Loss: 0.7435\n",
      "[Epoch 38/38] [Batch 0/469]D Loss: 1.2891 | G Loss: 0.7931\n",
      "[Epoch 38/38] [Batch 200/469]D Loss: 1.3511 | G Loss: 0.7846\n",
      "[Epoch 38/38] [Batch 400/469]D Loss: 1.1793 | G Loss: 0.9787\n",
      "[Epoch 39/39] [Batch 0/469]D Loss: 1.3469 | G Loss: 0.8877\n",
      "[Epoch 39/39] [Batch 200/469]D Loss: 1.3183 | G Loss: 0.8261\n",
      "[Epoch 39/39] [Batch 400/469]D Loss: 1.3934 | G Loss: 0.7529\n",
      "[Epoch 40/40] [Batch 0/469]D Loss: 1.1938 | G Loss: 0.9333\n",
      "[Epoch 40/40] [Batch 200/469]D Loss: 1.2396 | G Loss: 0.8852\n",
      "[Epoch 40/40] [Batch 400/469]D Loss: 1.4120 | G Loss: 0.8340\n",
      "[Epoch 41/41] [Batch 0/469]D Loss: 1.2595 | G Loss: 0.9112\n",
      "[Epoch 41/41] [Batch 200/469]D Loss: 1.1893 | G Loss: 0.9416\n",
      "[Epoch 41/41] [Batch 400/469]D Loss: 1.2928 | G Loss: 0.6390\n",
      "[Epoch 42/42] [Batch 0/469]D Loss: 1.3156 | G Loss: 0.7308\n",
      "[Epoch 42/42] [Batch 200/469]D Loss: 1.1983 | G Loss: 0.8772\n",
      "[Epoch 42/42] [Batch 400/469]D Loss: 1.2469 | G Loss: 1.0091\n",
      "[Epoch 43/43] [Batch 0/469]D Loss: 1.3202 | G Loss: 0.9663\n",
      "[Epoch 43/43] [Batch 200/469]D Loss: 1.2428 | G Loss: 0.9774\n",
      "[Epoch 43/43] [Batch 400/469]D Loss: 1.2498 | G Loss: 1.0028\n",
      "[Epoch 44/44] [Batch 0/469]D Loss: 1.3325 | G Loss: 0.8052\n",
      "[Epoch 44/44] [Batch 200/469]D Loss: 1.2562 | G Loss: 0.9052\n",
      "[Epoch 44/44] [Batch 400/469]D Loss: 1.2478 | G Loss: 1.0003\n",
      "[Epoch 45/45] [Batch 0/469]D Loss: 1.2865 | G Loss: 1.1930\n",
      "[Epoch 45/45] [Batch 200/469]D Loss: 1.1910 | G Loss: 0.9319\n",
      "[Epoch 45/45] [Batch 400/469]D Loss: 1.2047 | G Loss: 0.8612\n",
      "[Epoch 46/46] [Batch 0/469]D Loss: 1.3640 | G Loss: 0.9466\n",
      "[Epoch 46/46] [Batch 200/469]D Loss: 1.3135 | G Loss: 1.0086\n",
      "[Epoch 46/46] [Batch 400/469]D Loss: 1.2367 | G Loss: 1.0880\n",
      "[Epoch 47/47] [Batch 0/469]D Loss: 1.2197 | G Loss: 0.8876\n",
      "[Epoch 47/47] [Batch 200/469]D Loss: 1.2350 | G Loss: 1.0158\n",
      "[Epoch 47/47] [Batch 400/469]D Loss: 1.2337 | G Loss: 0.8582\n",
      "[Epoch 48/48] [Batch 0/469]D Loss: 1.1998 | G Loss: 1.1656\n",
      "[Epoch 48/48] [Batch 200/469]D Loss: 1.3034 | G Loss: 1.4865\n",
      "[Epoch 48/48] [Batch 400/469]D Loss: 1.2688 | G Loss: 1.1114\n",
      "[Epoch 49/49] [Batch 0/469]D Loss: 1.1483 | G Loss: 1.0209\n",
      "[Epoch 49/49] [Batch 200/469]D Loss: 1.2286 | G Loss: 1.0236\n",
      "[Epoch 49/49] [Batch 400/469]D Loss: 1.1711 | G Loss: 1.0621\n",
      "[Epoch 50/50] [Batch 0/469]D Loss: 1.1615 | G Loss: 1.0472\n",
      "[Epoch 50/50] [Batch 200/469]D Loss: 1.3714 | G Loss: 1.1785\n",
      "[Epoch 50/50] [Batch 400/469]D Loss: 1.1926 | G Loss: 1.2840\n"
     ]
    }
   ],
   "source": [
    "k, p = 3, 1 \n",
    "k = 3 # genrator updates per iterations\n",
    "p =1 # discriminator updates per iterations\n",
    "# train generator more than discriminator \n",
    "for epoch in range(1, epochs+1) : \n",
    "    for i, (real_imgs, real_labels) in enumerate(train_loader) :\n",
    "        batch_size_curr = real_imgs.size(0)\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "\n",
    "        real = torch.ones(batch_size_curr, 1, device=device)\n",
    "        fake = torch.zeros(batch_size_curr, 1, device=device)\n",
    "\n",
    "        for _ in range(p) : \n",
    "            z = torch.randn(batch_size_curr, z_dim, device=device)\n",
    "            fake_labels = torch.randint(0, num_classes, (batch_size_curr,), device=device)\n",
    "\n",
    "            with torch.no_grad(): \n",
    "                gen_imgs = generator(z, fake_labels)\n",
    "\n",
    "            real_validity = discriminator(real_imgs, real_labels)\n",
    "            d_real_loss = criterion(real_validity, real)\n",
    "\n",
    "            fake_validity = discriminator(gen_imgs.detach(), fake_labels)\n",
    "            d_fake_loss = criterion(fake_validity, fake)\n",
    "\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "        for _ in range(k) : \n",
    "            z = torch.randn(batch_size_curr, z_dim, device=device)\n",
    "            gen_labels = torch.randint(0, num_classes, (batch_size_curr,), device=device)\n",
    "            gen_imgs = generator(z, gen_labels)\n",
    "            \n",
    "            validity = discriminator(gen_imgs, gen_labels)\n",
    "            g_loss = criterion(validity, real)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "        \n",
    "        if i%200 == 0: \n",
    "            print(\n",
    "                f\"[Epoch {epoch}/{epoch}] [Batch {i}/{len(train_loader)}]\"\n",
    "                f\"D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(10, z_dim, device=device)\n",
    "        labels = torch.arange(0, 10, dtype=torch.long, device=device)\n",
    "        samples = generator(z, labels)\n",
    "        samples = samples*0.5 + 0.5\n",
    "        save_image(samples, f\"cgan_generated_images/epochs_{epoch}.png\", nrow=10)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a95f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_digit_images(generator, digit, num_samples = 16, save_path = None) : \n",
    "    generator.eval()\n",
    "    z = torch.randn(num_samples, z_dim).to(device=device)\n",
    "    labels = torch.full((num_samples,), digit, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad() : \n",
    "        gen_imgs = generator(z, labels)\n",
    "        gen_imgs = gen_imgs*0.5 + 0.5\n",
    "\n",
    "    if save_path : \n",
    "        save_image(gen_imgs, save_path, nrow=4)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "\n",
    "    return gen_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "362798a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to cgan_generated_images/seven.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[3.5384e-04, 5.9098e-05, 9.1341e-04,  ..., 2.1955e-04,\n",
       "           1.1718e-04, 8.7947e-05],\n",
       "          [3.6332e-04, 2.9445e-05, 1.4573e-04,  ..., 9.9570e-05,\n",
       "           1.0014e-05, 4.8280e-06],\n",
       "          [1.0499e-04, 3.6680e-02, 1.4390e-03,  ..., 7.4506e-07,\n",
       "           8.8811e-06, 3.6061e-05],\n",
       "          ...,\n",
       "          [3.5164e-04, 6.3897e-03, 1.5667e-03,  ..., 6.5681e-04,\n",
       "           5.0575e-05, 1.5825e-05],\n",
       "          [1.6093e-06, 2.3568e-04, 4.0731e-04,  ..., 1.7536e-04,\n",
       "           5.0122e-04, 2.4549e-02],\n",
       "          [1.4573e-05, 2.8413e-03, 2.9802e-07,  ..., 8.4089e-02,\n",
       "           1.5031e-03, 6.1452e-05]]],\n",
       "\n",
       "\n",
       "        [[[3.2851e-04, 5.4032e-05, 1.0688e-03,  ..., 2.6745e-04,\n",
       "           1.2723e-04, 8.8692e-05],\n",
       "          [3.6117e-04, 2.8998e-05, 1.5020e-04,  ..., 9.0867e-05,\n",
       "           1.1295e-05, 4.4703e-06],\n",
       "          [1.0625e-04, 3.6351e-02, 1.5407e-03,  ..., 8.3447e-07,\n",
       "           8.4937e-06, 4.4882e-05],\n",
       "          ...,\n",
       "          [4.2653e-04, 8.1497e-03, 1.6486e-03,  ..., 8.5306e-04,\n",
       "           5.5581e-05, 1.9580e-05],\n",
       "          [1.9968e-06, 2.8598e-04, 4.5690e-04,  ..., 1.7649e-04,\n",
       "           5.7864e-04, 2.2789e-02],\n",
       "          [1.4901e-05, 2.4969e-03, 3.5763e-07,  ..., 8.6346e-02,\n",
       "           1.9721e-03, 7.0095e-05]]],\n",
       "\n",
       "\n",
       "        [[[2.7987e-04, 4.0472e-05, 9.1407e-04,  ..., 2.7528e-04,\n",
       "           1.1882e-04, 9.0033e-05],\n",
       "          [3.7670e-04, 2.6405e-05, 1.3095e-04,  ..., 8.4192e-05,\n",
       "           1.1802e-05, 3.8147e-06],\n",
       "          [8.9586e-05, 3.4006e-02, 1.5378e-03,  ..., 8.3447e-07,\n",
       "           7.1526e-06, 4.5300e-05],\n",
       "          ...,\n",
       "          [4.1637e-04, 7.1320e-03, 1.5199e-03,  ..., 7.8747e-04,\n",
       "           4.9829e-05, 1.9044e-05],\n",
       "          [2.1458e-06, 2.3767e-04, 3.8338e-04,  ..., 1.7896e-04,\n",
       "           5.4395e-04, 2.2813e-02],\n",
       "          [1.5140e-05, 2.4668e-03, 3.5763e-07,  ..., 8.7316e-02,\n",
       "           1.8670e-03, 7.0751e-05]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[4.1014e-04, 4.7147e-05, 7.3135e-04,  ..., 1.9941e-04,\n",
       "           1.3053e-04, 8.5801e-05],\n",
       "          [3.2666e-04, 3.0100e-05, 1.3790e-04,  ..., 8.8751e-05,\n",
       "           1.1891e-05, 4.0233e-06],\n",
       "          [9.2417e-05, 2.5945e-02, 1.4952e-03,  ..., 5.9605e-07,\n",
       "           7.7784e-06, 3.8832e-05],\n",
       "          ...,\n",
       "          [4.4739e-04, 6.4702e-03, 2.1154e-03,  ..., 6.4531e-04,\n",
       "           3.9130e-05, 1.5855e-05],\n",
       "          [1.8477e-06, 2.7737e-04, 3.6290e-04,  ..., 1.8457e-04,\n",
       "           5.2479e-04, 2.1440e-02],\n",
       "          [1.3351e-05, 2.5164e-03, 3.5763e-07,  ..., 7.9032e-02,\n",
       "           1.6752e-03, 6.5774e-05]]],\n",
       "\n",
       "\n",
       "        [[[3.4222e-04, 4.7833e-05, 7.4199e-04,  ..., 2.3368e-04,\n",
       "           1.3372e-04, 8.9556e-05],\n",
       "          [3.1278e-04, 3.0994e-05, 1.4773e-04,  ..., 8.7976e-05,\n",
       "           1.1802e-05, 3.7253e-06],\n",
       "          [8.6516e-05, 2.9392e-02, 1.3933e-03,  ..., 6.5565e-07,\n",
       "           7.8082e-06, 3.6895e-05],\n",
       "          ...,\n",
       "          [4.4271e-04, 6.9181e-03, 1.9285e-03,  ..., 6.8286e-04,\n",
       "           4.0501e-05, 1.6510e-05],\n",
       "          [1.8775e-06, 2.7010e-04, 3.8290e-04,  ..., 1.8752e-04,\n",
       "           5.0777e-04, 2.3200e-02],\n",
       "          [1.2547e-05, 2.5086e-03, 3.2783e-07,  ..., 7.9575e-02,\n",
       "           1.6198e-03, 6.2764e-05]]],\n",
       "\n",
       "\n",
       "        [[[3.8797e-04, 5.1171e-05, 8.7985e-04,  ..., 2.2125e-04,\n",
       "           1.3143e-04, 9.1463e-05],\n",
       "          [3.4162e-04, 2.8491e-05, 1.4818e-04,  ..., 9.1374e-05,\n",
       "           1.2338e-05, 4.1723e-06],\n",
       "          [9.0510e-05, 3.3168e-02, 1.4365e-03,  ..., 6.8545e-07,\n",
       "           8.7321e-06, 4.0442e-05],\n",
       "          ...,\n",
       "          [4.5177e-04, 6.5017e-03, 1.9180e-03,  ..., 7.8988e-04,\n",
       "           4.2886e-05, 1.8269e-05],\n",
       "          [2.1160e-06, 2.7293e-04, 3.9199e-04,  ..., 1.9738e-04,\n",
       "           5.5930e-04, 2.2987e-02],\n",
       "          [1.4067e-05, 2.6965e-03, 3.8743e-07,  ..., 7.9814e-02,\n",
       "           1.8051e-03, 6.2078e-05]]]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_digit_images(generator, digit=7, num_samples=16, save_path='cgan_generated_images/seven.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8ae4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbec62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf2b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
